1.Missing values are empty entries in a dataset. They can be handled by removing rows/columns (dropna()), filling with statistical values (fillna()), or using imputation techniques. The choice depends on the data and problem context.
2.Duplicate records are repeated rows that can skew results. Use duplicated() to identify them and drop_duplicates() to remove. Always confirm if they are truly redundant before dropping.
3.dropna() removes rows or columns with missing values, while fillna() replaces missing entries with specified values like mean, median, or a constant. Use based on the nature and amount of missing data.
4.Outlier treatment involves handling extreme values that can distort analysis. Methods include removing, capping, or transforming outliers. It's crucial for building accurate models and reducing bias.
5.Standardizing data scales it to have a mean of 0 and standard deviation of 1. It ensures fair treatment of features in algorithms sensitive to feature scales, such as KNN or SVM.
6Inconsistent formats like date/time are handled using tools like pd.to_datetime(). Standardizing to a consistent format ensures uniformity and prevents parsing errors during analysis.
7.Common data cleaning challenges include missing data, inconsistent formats, outliers, duplicates, and typos. They require careful handling to maintain data accuracy and model quality.
8.To check data quality, inspect missing values, data types, duplicates, and summary stats. Use functions like info(), describe(), and value_counts() to assess completeness and consistency.
